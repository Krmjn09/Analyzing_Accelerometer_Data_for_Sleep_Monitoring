{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2a9b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.2)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/MAHAJAN/Documents/GitHub/Analyzing_Accelerometer_Data_for_Sleep_Monitoring/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn xgboost tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ba646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data\n",
    "train_series = pd.read_csv('converted_train_series.csv')\n",
    "train_events = pd.read_csv('train_events.csv')\n",
    "test_series = pd.read_csv('converted_test_series.csv')\n",
    "\n",
    "# Merge train_series and train_events\n",
    "train_data = pd.merge(train_series, train_events, on=['series_id', 'step'], how='left')\n",
    "train_data['event'].fillna('none', inplace=True)\n",
    "train_data['label'] = train_data['event'].apply(lambda x: 1 if x == 'onset' else (0 if x == 'wakeup' else -1))\n",
    "train_data = train_data[train_data['label'] != -1]\n",
    "\n",
    "# Feature Engineering\n",
    "train_data['angle_diff'] = train_data['anglez'].diff()\n",
    "train_data['anglez_squared'] = train_data['anglez'] ** 2\n",
    "train_data['log_enmo'] = np.log1p(train_data['enmo'])\n",
    "\n",
    "# Feature Selection\n",
    "features = ['anglez', 'enmo', 'angle_diff', 'anglez_squared', 'log_enmo']\n",
    "X = train_data[features]\n",
    "y = train_data['label']\n",
    "\n",
    "# === VISUALIZATION BEFORE PREPROCESSING ===\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Missing values\n",
    "print(\"Missing values in features:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Feature distributions\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.kdeplot(train_data[feature], shade=True, color='blue')\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.show()\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(train_data[features + ['label']], hue='label', palette='viridis')\n",
    "plt.suptitle(\"Pairplot of Features\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# === PREPROCESSING ===\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imputation\n",
    "print(\"\\n=== Step 1: Imputation ===\")\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Feature Scaling\n",
    "print(\"\\n=== Step 2: Scaling ===\")\n",
    "scaler = StandardScaler()  # You can experiment with other scalers like MinMaxScaler\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Power Transformation\n",
    "print(\"\\n=== Step 3: Power Transformation ===\")\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "X_transformed = transformer.fit_transform(X_scaled)\n",
    "\n",
    "# Polynomial Feature Expansion\n",
    "print(\"\\n=== Step 4: Polynomial Feature Expansion ===\")\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_transformed)\n",
    "\n",
    "# Feature Selection using Mutual Information\n",
    "print(\"\\n=== Step 5: Feature Selection ===\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_selected = selector.fit_transform(X_poly, y)\n",
    "\n",
    "# Selected feature names\n",
    "feature_names = poly.get_feature_names_out(features)\n",
    "selected_features = feature_names[selector.get_support()]\n",
    "print(\"Selected features after feature selection:\")\n",
    "print(selected_features)\n",
    "\n",
    "# === DATA SPLIT ===\n",
    "print(\"\\n=== Step 6: Splitting Data ===\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check if any feature has NaN or Inf values after transformation\n",
    "import numpy as np\n",
    "print(\"\\n=== Step 7: Checking NaN or Inf values ===\")\n",
    "print(\"Any NaN in X_train:\", np.any(np.isnan(X_train)))\n",
    "print(\"Any Inf in X_train:\", np.any(np.isinf(X_train)))\n",
    "print(\"Any NaN in X_val:\", np.any(np.isnan(X_val)))\n",
    "print(\"Any Inf in X_val:\", np.any(np.isinf(X_val)))\n",
    "\n",
    "# === Further Processing ===\n",
    "# Optional: Check for multicollinearity if you want to drop highly correlated features\n",
    "import pandas as pd\n",
    "corr_matrix = pd.DataFrame(X_train).corr()\n",
    "print(\"\\nCorrelation matrix of features:\")\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c98699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights\n",
    "def compute_weights(y_train):\n",
    "    \"\"\"Compute class weights for imbalanced datasets.\"\"\"\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    return {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# Compute class weights\n",
    "class_weight_dict = compute_weights(y_train)\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2fc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130bde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Custom scorer that combines F1 and accuracy\n",
    "def combined_scorer(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return 0.7 * f1 + 0.3 * acc  # Weighted combination favoring F1\n",
    "\n",
    "# Enhanced function to define models with expanded parameter grids\n",
    "def define_models(class_weight_dict):\n",
    "    \"\"\"Define machine learning models with expanded hyperparameter grids.\"\"\"\n",
    "    return {\n",
    "        'XGBoost': (XGBClassifier(random_state=42, eval_metric='logloss', tree_method='gpu_hist'), {\n",
    "            'n_estimators': [100, 200, 300, 400],\n",
    "            'max_depth': [3, 5, 7, 9, 11],\n",
    "            'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'scale_pos_weight': [class_weight_dict[1]/class_weight_dict[0]]\n",
    "        }),\n",
    "        'Logistic Regression': (LogisticRegression(random_state=42, max_iter=5000), {\n",
    "            'C': np.logspace(-4, 4, 20),\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'solver': ['saga'],\n",
    "            'l1_ratio': [0, 0.25, 0.5, 0.75, 1] if 'elasticnet' in ['l1', 'l2', 'elasticnet'] else [None],\n",
    "            'class_weight': [class_weight_dict, 'balanced']\n",
    "        }),\n",
    "        'Random Forest': (RandomForestClassifier(random_state=42), {\n",
    "            'n_estimators': [100, 200, 300, 400],\n",
    "            'max_depth': [None, 10, 20, 30, 50],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', 0.5, 0.8],\n",
    "            'bootstrap': [True, False],\n",
    "            'class_weight': ['balanced', 'balanced_subsample', class_weight_dict]\n",
    "        }),\n",
    "        'SVM': (SVC(probability=True, random_state=42), {\n",
    "            'C': np.logspace(-2, 3, 20),\n",
    "            'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "            'gamma': ['scale', 'auto'] + list(np.logspace(-3, 1, 10)),\n",
    "            'class_weight': [class_weight_dict, 'balanced']\n",
    "        }),\n",
    "        'ExtraTrees': (ExtraTreesClassifier(random_state=42), {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'max_features': ['sqrt', 'log2', 0.5, 0.8],\n",
    "            'class_weight': ['balanced', class_weight_dict]\n",
    "        })\n",
    "    }\n",
    "\n",
    "# Enhanced training function with more folds and early stopping\n",
    "def train_models(models, X_train, y_train, cv_splits=15, scoring=make_scorer(combined_scorer)):\n",
    "    \"\"\"Train models using increased CV folds and optimized search.\"\"\"\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "    best_models = {}\n",
    "    \n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        logging.info(f\"\\n=== Training {model_name} with {cv_splits}-fold CV ===\")\n",
    "        \n",
    "        # Use RandomizedSearchCV with more iterations\n",
    "        search = RandomizedSearchCV(\n",
    "            model, param_grid, \n",
    "            n_iter=50,  # Increased from 20\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "        # Enhanced evaluation\n",
    "        cv_scores = cross_val_score(\n",
    "            search.best_estimator_, \n",
    "            X_train, y_train, \n",
    "            cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42),\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Best {model_name} params: {search.best_params_}\")\n",
    "        logging.info(f\"10-fold CV Score: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")\n",
    "        \n",
    "        # Feature importance for tree-based models\n",
    "        if hasattr(search.best_estimator_, 'feature_importances_'):\n",
    "            importances = search.best_estimator_.feature_importances_\n",
    "            logging.info(f\"Top 5 features: {sorted(zip(features, importances), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "# Usage\n",
    "class_weight_dict = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weight_dict[0], 1: class_weight_dict[1]}\n",
    "\n",
    "models = define_models(class_weight_dict)\n",
    "best_models = train_models(models, X_train, y_train, cv_splits=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbeebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to evaluate a model and return metrics\n",
    "def evaluate_model(model, X_val, y_val, is_keras=False):\n",
    "    \"\"\"\n",
    "    Evaluate a model and return accuracy, precision, recall, and F1-score.\n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        X_val: Validation features.\n",
    "        y_val: Validation labels.\n",
    "        is_keras: Boolean indicating if the model is a Keras model.\n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    if is_keras:\n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(int).flatten()\n",
    "    else:\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='macro')\n",
    "    recall = recall_score(y_val, y_pred, average='macro')\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "\n",
    "# Select the best model based on validation F1 score\n",
    "if best_models:\n",
    "    # Dictionary to store evaluation results for all models\n",
    "    evaluation_results = {}\n",
    "\n",
    "    # Evaluate each model and log its metrics\n",
    "    for name, model in best_models.items():\n",
    "        is_keras = isinstance(model, tf.keras.Model)\n",
    "        metrics = evaluate_model(model, X_val_seq if is_keras else X_val, y_val_seq.flatten() if is_keras else y_val, is_keras=is_keras)\n",
    "        \n",
    "        # Store the evaluation results\n",
    "        evaluation_results[name] = metrics\n",
    "        \n",
    "        # Log the evaluation metrics for the current model\n",
    "        logging.info(f\"\\n=== Evaluation Results for {name} ===\")\n",
    "        logging.info(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "        logging.info(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "        logging.info(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "        logging.info(f\"F1-Score: {metrics['F1-Score']:.4f}\")\n",
    "\n",
    "    # Select the best model based on F1-score\n",
    "    final_model_name, best_metrics = max(\n",
    "        evaluation_results.items(),\n",
    "        key=lambda x: x[1]['F1-Score']  # Use F1-score as the key for comparison\n",
    "    )\n",
    "    final_model = best_models[final_model_name]\n",
    "\n",
    "    # Log the best model and its metrics\n",
    "    logging.info(f\"\\n=== Selected Best Model ===\")\n",
    "    logging.info(f\"Model: {final_model.__class__.__name__}\")\n",
    "    logging.info(f\"Accuracy: {best_metrics['Accuracy']:.4f}\")\n",
    "    logging.info(f\"Precision: {best_metrics['Precision']:.4f}\")\n",
    "    logging.info(f\"Recall: {best_metrics['Recall']:.4f}\")\n",
    "    logging.info(f\"F1-Score: {best_metrics['F1-Score']:.4f}\")\n",
    "else:\n",
    "    logging.warning(\"No models were successfully trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare predictions for submission\n",
    "test_data = test_series.copy()\n",
    "test_data['angle_diff'] = test_data['anglez'].diff()\n",
    "test_data['anglez_squared'] = test_data['anglez'] ** 2\n",
    "test_data['log_enmo'] = np.log1p(test_data['enmo'])\n",
    "X_test = test_data[features]\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "X_test_selected = selector.transform(X_test_poly)\n",
    "\n",
    "# Predict using the best model\n",
    "if isinstance(final_model, tf.keras.Model):\n",
    "    X_test_seq, _ = create_sequences(X_test_selected, np.zeros(len(X_test_selected)), sequence_length)\n",
    "    test_predictions = (final_model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "else:\n",
    "    test_predictions = final_model.predict(X_test_selected)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': range(len(test_series)),\n",
    "    'series_id': test_series['series_id'],\n",
    "    'step': test_series['step'],\n",
    "    'event': ['onset' if pred == 1 else 'wakeup' if pred == 0 else 'none' for pred in test_predictions],\n",
    "    'score': [max(prob) for prob in final_model.predict_proba(X_test_selected)] if hasattr(final_model, 'predict_proba') else [0.5] * len(test_series)\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission_final.csv', index=False)\n",
    "print(\"Submission file created: submission_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Step 1: Load Submission File ===\n",
    "# Assuming the submission file is already created and named 'submission_final.csv'\n",
    "submission_df = pd.read_csv('submission_final.csv')\n",
    "\n",
    "# Filter rows where the event is either 'onset' or 'wakeup'\n",
    "submission_filtered = submission_df[submission_df['event'].isin(['onset', 'wakeup'])]\n",
    "\n",
    "# === Step 2: Evaluate Confidence Scores ===\n",
    "print(\"\\n=== Confidence Score Summary ===\")\n",
    "\n",
    "# Group by event type and calculate summary statistics\n",
    "confidence_summary = submission_filtered.groupby('event')['score'].agg(['mean', 'median', 'min', 'max', 'std'])\n",
    "print(confidence_summary)\n",
    "\n",
    "# === Step 3: Visualize Confidence Score Distributions ===\n",
    "print(\"\\n=== Visualizing Confidence Score Distributions ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Confidence scores for 'onset'\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(submission_filtered[submission_filtered['event'] == 'onset']['score'], bins=30, kde=True, color='blue')\n",
    "plt.title('Confidence Scores for Onset Events')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Confidence scores for 'wakeup'\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(submission_filtered[submission_filtered['event'] == 'wakeup']['score'], bins=30, kde=True, color='orange')\n",
    "plt.title('Confidence Scores for Wakeup Events')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "# After preparing the final submission\n",
    "submission.to_csv('submission_final.csv', index=False)\n",
    "print(\"Submission file created: submission_final.csv\")\n",
    "\n",
    "# Save the final model\n",
    "if isinstance(final_model, tf.keras.Model):\n",
    "    # Save Keras model\n",
    "    final_model.save('final_keras_model.h5')  # Saves the model in H5 format\n",
    "    print(\"Keras model saved as 'final_keras_model.h5'\")\n",
    "else:\n",
    "    # Save scikit-learn model\n",
    "    joblib.dump(final_model, 'final_sklearn_model.pkl')  # Saves the model in pickle format\n",
    "    print(\"scikit-learn model saved as 'final_sklearn_model.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
